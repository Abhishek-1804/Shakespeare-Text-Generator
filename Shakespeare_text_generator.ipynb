{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2gefdc-iiu4"
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DkihvZ9tF_p7"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, LSTM, Dense, Embedding, Dropout, GRU\n",
    "\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nBUoGY_DilIK"
   },
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "GBnXa55mGV1_"
   },
   "outputs": [],
   "source": [
    "path_to_file = './shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "051EodVfGcMi"
   },
   "outputs": [],
   "source": [
    "text = open(path_to_file,'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UO38WGQPGd-c",
    "outputId": "3d105237-2479-494b-aff0-27f4630d0317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGYTafxsGha3",
    "outputId": "d689e934-b7f3-4af6-c611-54ac7832996b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ld may see my pleasure,\n",
      "  Sometime all full with feasting on your sight,\n",
      "  And by and by clean starved for a look,\n",
      "  Possessing or pursuing no delight\n",
      "  Save what is had, or must from you be took.\n",
      "    Thus do I pine and surfeit day by day,\n",
      "    Or gluttoning on all, or all away.\n",
      "\n",
      "\n",
      "                     76  \n",
      "  Why is my verse so barren of new pride?\n",
      "  So far from variation or quick change?\n",
      "  Why with the time do I not glance aside\n",
      "  To new-found methods, and to compounds strange?\n",
      "  Why write I still all one, ever the same,\n",
      "  And keep invention in a noted weed,\n",
      "  That every word doth almost tell m\n"
     ]
    }
   ],
   "source": [
    "print(text[50000:50600])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qm9qHB4jipdo"
   },
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0APCXABGkcE",
    "outputId": "e7e41dfc-d677-40f7-bb65-cbb6ae071ec1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xZBcCh9wGwtU"
   },
   "outputs": [],
   "source": [
    "char_to_ind = {char:ind for ind,char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TEuhsAATGzOb"
   },
   "outputs": [],
   "source": [
    "ind_to_char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Zs6jgNYIG1nL"
   },
   "outputs": [],
   "source": [
    "encoded_text = [char_to_ind[s] for s in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FcYuI3dWG3jx"
   },
   "outputs": [],
   "source": [
    "lines = '''\n",
    "Sometime all full with feasting on your sight,\n",
    "  And by and by clean starved for a look,\n",
    "  Possessing or pursuing no delight\n",
    "  Save what is had, or must from you be took.\n",
    "    Thus do I pine and surfeit day by day,\n",
    "    Or gluttoning on all, or all away.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0RGP2KpGHU4N",
    "outputId": "76740f71-09e0-486b-de89-2c358d1effde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "E1Kjf_yUHVlE"
   },
   "outputs": [],
   "source": [
    "seq_len = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_BXR1KB8HZEJ",
    "outputId": "79c4e2c3-58e6-4b00-ef9d-7f9d55a329e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21695"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_num_seq = len(text)//(seq_len + 1)\n",
    "total_num_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RX-ME78MHaoY",
    "outputId": "da84cc3c-3661-4372-cf75-88d85985a821"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 07:20:15.510894: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2025-10-16 07:20:15.510949: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2025-10-16 07:20:15.510956: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.92 GB\n",
      "2025-10-16 07:20:15.511012: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-10-16 07:20:15.511049: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "len(char_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLcW4YHEH0dE",
    "outputId": "732fae6f-7329-4cb6-de50-3480c11ec388"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=TensorSpec(shape=(251,), dtype=tf.int32, name=None)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_len+1, drop_remainder=True)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HI55mnCaH5LA"
   },
   "outputs": [],
   "source": [
    "def create_seq_targets(seq):\n",
    "    input_txt = seq[:-1]\n",
    "    target_txt = seq[1:]\n",
    "    return input_txt, target_txt\n",
    "\n",
    "dataset = sequences.map(create_seq_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjEBZhKxi_To"
   },
   "source": [
    "# Creating LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gXlmjr2PIBPq"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# Buffer size to shuffle the dataset so it doesn't attempt to shuffle\n",
    "# the entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements\n",
    "buffer_size = 8000\n",
    "\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1zeSIhpqIEEQ",
    "outputId": "637414d4-8d41-47e2-868e-3975b3970c4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_BatchDataset element_spec=(TensorSpec(shape=(128, 250), dtype=tf.int32, name=None), TensorSpec(shape=(128, 250), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7e2uQ7DgI8eW"
   },
   "outputs": [],
   "source": [
    "def sparse_cat_loss(y_true,y_pred):\n",
    "  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) #Onehotencoding is done so from_logits = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "5PfBhxpRIFtC"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embed_dim = 84\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_neurons = 1026"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5shoJapXI-oX"
   },
   "outputs": [],
   "source": [
    "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(batch_input_shape=(batch_size, None)))   # define static batch\n",
    "    model.add(Embedding(vocab_size, embed_dim))\n",
    "    model.add(LSTM(rnn_neurons, return_sequences=True, stateful=True,\n",
    "                   recurrent_initializer='glorot_uniform', dropout=0.4))\n",
    "    model.add(LSTM(500, return_sequences=True, stateful=True,\n",
    "                   recurrent_initializer='glorot_uniform', dropout=0.4))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "9LT3LiYHJBF6"
   },
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embed_dim=embed_dim,\n",
    "  rnn_neurons=rnn_neurons,\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DR77dsiXJDAx",
    "outputId": "56f0ed6c-2921-4be7-9ec0-1b7f60e6cefd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (128, None, 84)           7056      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (128, None, 1026)         4559544   \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (128, None, 500)          3054000   \n",
      "                                                                 \n",
      " dense (Dense)               (128, None, 84)           42084     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7662684 (29.23 MB)\n",
      "Trainable params: 7662684 (29.23 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKraaL_RkJ_D"
   },
   "source": [
    "## Example predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwnwz_HGiDj-",
    "outputId": "7579f18f-7681-4415-f421-6bbdfd08b005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 250, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "\n",
    "  # Predict off some random batch\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "\n",
    "  # Display the dimensions of the predictions\n",
    "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdCqIf5MiEnG"
   },
   "source": [
    "# Example batch predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5HnlTvXuiIxP",
    "outputId": "11c0d974-2d81-4b68-ef10-26b5f7fcddbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(250, 1), dtype=int64, numpy=\n",
       "array([[29],\n",
       "       [12],\n",
       "       [ 7],\n",
       "       [59],\n",
       "       [ 8],\n",
       "       [41],\n",
       "       [33],\n",
       "       [82],\n",
       "       [75],\n",
       "       [29],\n",
       "       [31],\n",
       "       [25],\n",
       "       [22],\n",
       "       [12],\n",
       "       [76],\n",
       "       [ 0],\n",
       "       [34],\n",
       "       [45],\n",
       "       [18],\n",
       "       [81],\n",
       "       [31],\n",
       "       [ 2],\n",
       "       [14],\n",
       "       [76],\n",
       "       [48],\n",
       "       [16],\n",
       "       [67],\n",
       "       [ 1],\n",
       "       [38],\n",
       "       [66],\n",
       "       [ 7],\n",
       "       [51],\n",
       "       [27],\n",
       "       [16],\n",
       "       [15],\n",
       "       [75],\n",
       "       [39],\n",
       "       [22],\n",
       "       [70],\n",
       "       [22],\n",
       "       [39],\n",
       "       [34],\n",
       "       [82],\n",
       "       [59],\n",
       "       [16],\n",
       "       [ 2],\n",
       "       [81],\n",
       "       [39],\n",
       "       [17],\n",
       "       [33],\n",
       "       [44],\n",
       "       [54],\n",
       "       [34],\n",
       "       [52],\n",
       "       [40],\n",
       "       [43],\n",
       "       [47],\n",
       "       [ 9],\n",
       "       [28],\n",
       "       [22],\n",
       "       [60],\n",
       "       [13],\n",
       "       [36],\n",
       "       [10],\n",
       "       [74],\n",
       "       [44],\n",
       "       [46],\n",
       "       [68],\n",
       "       [24],\n",
       "       [23],\n",
       "       [52],\n",
       "       [60],\n",
       "       [45],\n",
       "       [51],\n",
       "       [35],\n",
       "       [52],\n",
       "       [44],\n",
       "       [42],\n",
       "       [51],\n",
       "       [77],\n",
       "       [23],\n",
       "       [56],\n",
       "       [31],\n",
       "       [22],\n",
       "       [65],\n",
       "       [20],\n",
       "       [63],\n",
       "       [38],\n",
       "       [ 7],\n",
       "       [21],\n",
       "       [53],\n",
       "       [13],\n",
       "       [27],\n",
       "       [42],\n",
       "       [57],\n",
       "       [78],\n",
       "       [37],\n",
       "       [12],\n",
       "       [ 5],\n",
       "       [23],\n",
       "       [33],\n",
       "       [40],\n",
       "       [60],\n",
       "       [66],\n",
       "       [55],\n",
       "       [77],\n",
       "       [54],\n",
       "       [59],\n",
       "       [64],\n",
       "       [67],\n",
       "       [35],\n",
       "       [45],\n",
       "       [70],\n",
       "       [34],\n",
       "       [41],\n",
       "       [17],\n",
       "       [10],\n",
       "       [69],\n",
       "       [17],\n",
       "       [11],\n",
       "       [ 2],\n",
       "       [16],\n",
       "       [65],\n",
       "       [74],\n",
       "       [81],\n",
       "       [13],\n",
       "       [59],\n",
       "       [36],\n",
       "       [79],\n",
       "       [53],\n",
       "       [82],\n",
       "       [47],\n",
       "       [ 3],\n",
       "       [34],\n",
       "       [ 6],\n",
       "       [52],\n",
       "       [13],\n",
       "       [55],\n",
       "       [33],\n",
       "       [41],\n",
       "       [79],\n",
       "       [23],\n",
       "       [32],\n",
       "       [25],\n",
       "       [48],\n",
       "       [52],\n",
       "       [ 8],\n",
       "       [61],\n",
       "       [32],\n",
       "       [ 4],\n",
       "       [12],\n",
       "       [76],\n",
       "       [49],\n",
       "       [ 6],\n",
       "       [26],\n",
       "       [20],\n",
       "       [59],\n",
       "       [66],\n",
       "       [35],\n",
       "       [ 6],\n",
       "       [35],\n",
       "       [49],\n",
       "       [57],\n",
       "       [50],\n",
       "       [35],\n",
       "       [20],\n",
       "       [58],\n",
       "       [42],\n",
       "       [59],\n",
       "       [ 0],\n",
       "       [48],\n",
       "       [ 2],\n",
       "       [46],\n",
       "       [11],\n",
       "       [ 6],\n",
       "       [10],\n",
       "       [82],\n",
       "       [49],\n",
       "       [59],\n",
       "       [ 4],\n",
       "       [19],\n",
       "       [71],\n",
       "       [30],\n",
       "       [73],\n",
       "       [14],\n",
       "       [29],\n",
       "       [27],\n",
       "       [19],\n",
       "       [15],\n",
       "       [12],\n",
       "       [17],\n",
       "       [14],\n",
       "       [56],\n",
       "       [19],\n",
       "       [77],\n",
       "       [73],\n",
       "       [51],\n",
       "       [50],\n",
       "       [27],\n",
       "       [71],\n",
       "       [10],\n",
       "       [14],\n",
       "       [68],\n",
       "       [63],\n",
       "       [ 6],\n",
       "       [74],\n",
       "       [ 7],\n",
       "       [21],\n",
       "       [76],\n",
       "       [72],\n",
       "       [54],\n",
       "       [67],\n",
       "       [57],\n",
       "       [49],\n",
       "       [59],\n",
       "       [ 2],\n",
       "       [35],\n",
       "       [45],\n",
       "       [ 4],\n",
       "       [67],\n",
       "       [54],\n",
       "       [73],\n",
       "       [56],\n",
       "       [51],\n",
       "       [51],\n",
       "       [15],\n",
       "       [66],\n",
       "       [66],\n",
       "       [31],\n",
       "       [ 8],\n",
       "       [25],\n",
       "       [32],\n",
       "       [82],\n",
       "       [55],\n",
       "       [77],\n",
       "       [45],\n",
       "       [19],\n",
       "       [48],\n",
       "       [42],\n",
       "       [66],\n",
       "       [76],\n",
       "       [81],\n",
       "       [ 1],\n",
       "       [17],\n",
       "       [43],\n",
       "       [ 6],\n",
       "       [75],\n",
       "       [ 3],\n",
       "       [61],\n",
       "       [33]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ow1TuePxiKlp",
    "outputId": "a3a5354f-1971-45f1-9b36-8137bbea7810"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 12,  7, 59,  8, 41, 33, 82, 75, 29, 31, 25, 22, 12, 76,  0, 34,\n",
       "       45, 18, 81, 31,  2, 14, 76, 48, 16, 67,  1, 38, 66,  7, 51, 27, 16,\n",
       "       15, 75, 39, 22, 70, 22, 39, 34, 82, 59, 16,  2, 81, 39, 17, 33, 44,\n",
       "       54, 34, 52, 40, 43, 47,  9, 28, 22, 60, 13, 36, 10, 74, 44, 46, 68,\n",
       "       24, 23, 52, 60, 45, 51, 35, 52, 44, 42, 51, 77, 23, 56, 31, 22, 65,\n",
       "       20, 63, 38,  7, 21, 53, 13, 27, 42, 57, 78, 37, 12,  5, 23, 33, 40,\n",
       "       60, 66, 55, 77, 54, 59, 64, 67, 35, 45, 70, 34, 41, 17, 10, 69, 17,\n",
       "       11,  2, 16, 65, 74, 81, 13, 59, 36, 79, 53, 82, 47,  3, 34,  6, 52,\n",
       "       13, 55, 33, 41, 79, 23, 32, 25, 48, 52,  8, 61, 32,  4, 12, 76, 49,\n",
       "        6, 26, 20, 59, 66, 35,  6, 35, 49, 57, 50, 35, 20, 58, 42, 59,  0,\n",
       "       48,  2, 46, 11,  6, 10, 82, 49, 59,  4, 19, 71, 30, 73, 14, 29, 27,\n",
       "       19, 15, 12, 17, 14, 56, 19, 77, 73, 51, 50, 27, 71, 10, 14, 68, 63,\n",
       "        6, 74,  7, 21, 76, 72, 54, 67, 57, 49, 59,  2, 35, 45,  4, 67, 54,\n",
       "       73, 56, 51, 51, 15, 66, 66, 31,  8, 25, 32, 82, 55, 77, 45, 19, 48,\n",
       "       42, 66, 76, 81,  1, 17, 43,  6, 75,  3, 61, 33])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reformat to not be a lists of lists\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u7SShbtIiPcl",
    "outputId": "cfae75f1-dc8b-492c-92e5-695b2db46da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the input seq: \n",
      "\n",
      "hy sting is not so sharp\n",
      "              As friend rememb'red not.\n",
      "    Heigh-ho! sing, &c.\n",
      "\n",
      "  DUKE SENIOR. If that you were the good Sir Rowland's son,\n",
      "    As you have whisper'd faithfully you were,\n",
      "    And as mine eye doth his effigies witness\n",
      "    Mos\n",
      "\n",
      "\n",
      "Next Char Predictions: \n",
      "\n",
      "D1)d,PH|tDF?;1u\n",
      "IT7zF!3uW5l Mk)ZB54tN;o;NI|d5!zN6HS_I[ORV-C;e2K.sSUm><[eTZJ[SQZv<aF;j9hM):]2BQbwL1'<HOek`v_dilJToIP6.n60!5jsz2dKx]|V\"I([2`HPx<G?W[,fG&1uX(A9dkJ(JXbYJ9cQd\n",
      "W!U0(.|Xd&8pEr3DB84163a8vrZYBp.3mh(s):uq_lbXd!JT&l_raZZ4kkF,?G|`vT8WQkuz 6R(t\"fH\n"
     ]
    }
   ],
   "source": [
    "print(\"Given the input seq: \\n\")\n",
    "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
    "print('\\n')\n",
    "print(\"Next Char Predictions: \\n\")\n",
    "print(\"\".join(ind_to_char[sampled_indices ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rQS5N4PkO2x"
   },
   "source": [
    "# Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cEMm24BeJE99",
    "outputId": "714c6a42-f6d8-4ab7-e45d-94a28f667c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:65\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     67\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/keras/src/engine/training.py:1783\u001b[39m, in \u001b[36mModel.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[39m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tf.profiler.experimental.Trace(\n\u001b[32m   1776\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1777\u001b[39m     epoch_num=epoch,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1780\u001b[39m     _r=\u001b[32m1\u001b[39m,\n\u001b[32m   1781\u001b[39m ):\n\u001b[32m   1782\u001b[39m     callbacks.on_train_batch_begin(step)\n\u001b[32m-> \u001b[39m\u001b[32m1783\u001b[39m     tmp_logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1784\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data_handler.should_sync:\n\u001b[32m   1785\u001b[39m         context.async_wait()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:831\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    828\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m831\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    834\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:867\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    864\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    865\u001b[39m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[32m    866\u001b[39m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m867\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    871\u001b[39m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[32m    872\u001b[39m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[32m    873\u001b[39m   \u001b[38;5;28mself\u001b[39m._lock.release()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1264\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1260\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1262\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1263\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1266\u001b[39m     args,\n\u001b[32m   1267\u001b[39m     possible_gradient_type,\n\u001b[32m   1268\u001b[39m     executing_eagerly)\n\u001b[32m   1269\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:217\u001b[39m, in \u001b[36mAtomicFunction.flat_call\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mflat_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    216\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:252\u001b[39m, in \u001b[36mAtomicFunction.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    251\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    258\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    261\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    262\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1479\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1477\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1479\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1480\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1481\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1482\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1485\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1486\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1487\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1488\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1489\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1493\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1494\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Shakespeare-Text-Generator/.venv/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:60\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     53\u001b[39m   \u001b[38;5;66;03m# Convert any objects of type core_types.Tensor to Tensor.\u001b[39;00m\n\u001b[32m     54\u001b[39m   inputs = [\n\u001b[32m     55\u001b[39m       tensor_conversion_registry.convert(t)\n\u001b[32m     56\u001b[39m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, core_types.Tensor)\n\u001b[32m     57\u001b[39m       \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[32m     58\u001b[39m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[32m     59\u001b[39m   ]\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     63\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model.fit(dataset,epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdSHCfmWkVpR"
   },
   "source": [
    "# Saving model as .h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "vlSQexO_Kihm"
   },
   "outputs": [],
   "source": [
    "model.save('shakespeare_gen1.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "m-bh8SxPKzQd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "UKGQx94-K0zB"
   },
   "outputs": [],
   "source": [
    "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)\n",
    "\n",
    "model.load_weights('shakespeare_gen1.h5')\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dggbbxeokZMy"
   },
   "source": [
    "#Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "UcSIFbF7K18L"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_seed,gen_size=100,temp=1.0):\n",
    "  '''\n",
    "  model: Trained Model to Generate Text\n",
    "  start_seed: Intial Seed text in string form\n",
    "  gen_size: Number of characters to generate\n",
    "\n",
    "  Basic idea behind this function is to take in some seed text, format it so\n",
    "  that it is in the correct shape for our network, then loop the sequence as\n",
    "  we keep adding our own predicted characters. Similar to our work in the RNN\n",
    "  time series problems.\n",
    "  '''\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = gen_size\n",
    "\n",
    "  # Vecotrizing starting seed text\n",
    "  input_eval = [char_to_ind[s] for s in start_seed]\n",
    "\n",
    "  # Expand to match batch format shape\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty list to hold resulting generated text\n",
    "  text_generated = []\n",
    "\n",
    "  # Temperature effects randomness in our resulting text\n",
    "  # The term is derived from entropy/thermodynamics.\n",
    "  # The temperature is used to effect probability of next characters.\n",
    "  # Higher probability == lesss surprising/ more expected\n",
    "  # Lower temperature == more surprising / less expected\n",
    " \n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "\n",
    "  for i in range(num_generate):\n",
    "\n",
    "      # Generate Predictions\n",
    "      predictions = model(input_eval)\n",
    "\n",
    "      # Remove the batch shape dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # Use a cateogircal disitribution to select the next character\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # Pass the predicted charracter for the next input\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      # Transform back to character letter\n",
    "      text_generated.append(ind_to_char[predicted_id])\n",
    "\n",
    "  return (start_seed + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HpDjGWfPK7HK",
    "outputId": "1dc20d22-e0e9-4cdb-bc1c-9362e24592a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JULIET AND             Exit. Soldiers.\n",
      "\n",
      "          Enter PROTEUS, VALENTINE, and SHYLOCK\n",
      "\n",
      "               EO-enter CHARMIAN, IACHIO, AUMERLE, CHILD and ATTENDANTS\n",
      "\n",
      "  CLARENCE. O, let me sing your Grace!\n",
      "    What, art thou to our conscience?\n",
      "  MENELAUS. If I can rush so well,\n",
      "    Impromish your equisore.\n",
      "  LEONTES. Come, come, pardon; let 't it down.\n",
      "  NESTOR. Your power great Priam shall.                  [Drum forth]\n",
      "  IACHIMO.                    Thank you so hung?  \n",
      "  AARON. How would you then depart at from your Grace?\n",
      "  GLOUCESTER. How bashful and Troy. O Caesar, I dare hear\n",
      "    Though given to sport, cross-gill'd and bloody wearth!\n",
      "  EDWARD. Even here unsadled Warwick give you jot;\n",
      "             The combin of the world able how\n",
      "                  As false against the fool.\n",
      "                  Ho! \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model,\"JULIET \",gen_size=800))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOLHibGdn+6mCAN4Vcg1ccY",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1nzJNtVln3WBJSTjyNpBXdbakLzYPoBBb",
   "name": "Shakespeare text generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
